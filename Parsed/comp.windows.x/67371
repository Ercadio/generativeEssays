From bnoble Brian Noble Subject X Server scanline padding question I am almost done porting to a new piece of display hardware but have run into a snag I think may be somewhat commonplace so I'm sending a net I have a display that is a non interlaced memory mapped bit The server's view of the world obtained via xwd xwud seems to be exactly what it should However the displayed version of the framebuffer gives the impression that the server is using scanlines that are too After a bit of experimentation it seems that the problem was that the server was padding the line out to a word boundry but the scanline size in the buffer is bytes which isn't exactly divisible by Changing the following defines in mit server include define BITMAP SCANLINE PAD define BITMAP PAD define BYTES PER SCANLINE PAD to define BITMAP SCANLINE PAD define BITMAP PAD define BYTES PER SCANLINE PAD Was not exactly the right How do I tell the server either a don't pad the scan lines at all 'cause this server is only being built to run on this particular display or to pad only to byte boundries I'm using a customized version of under Mach Thanks Brian